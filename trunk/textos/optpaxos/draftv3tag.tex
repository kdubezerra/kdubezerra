\documentclass[times, 10pt]{article} 
%\documentclass[times, 10pt,twocolumn]{article} 
%\usepackage[noend]{distribalgo}
\usepackage{algorithm}
%\usepackage{times}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage[noend]{distribalgo}
\usepackage[draft]{fixme}
\usepackage[hmargin=2cm,vmargin=2cm]{geometry}


\begin{document}

\newcommand{\mv}[1]{\ensuremath{\operatorname{\mathit{#1}}}}
\definecolor{dark}{gray}{.6}
\newcommand{\bc}[1]{\textcolor{dark}{#1}}
\newtheorem{lems}{Lemma}
\newtheorem{props}{Proposition}
\newtheorem{thms}{Theorem}
\newtheorem{defs}{Definition}
\newtheorem{obs}{Observation}

\newcommand{\code}[1]{\texttt{\small{\textbf{#1}}}}

\newcommand{\blankline}{\vspace{4 mm}}
\newcommand{\cms}{\mbox{QGFTO-Mcast}}
\newcommand{\cmsend}[1]{\mbox{\cms({#1})}}
\newcommand{\tconsm}{T_{cons}}
\newcommand{\tcons}{\mbox{$\tconsm$}}
\newcommand{\opt}{\mbox{GF-OPT-Deliver}}
\newcommand{\cons}{\mbox{QGFTO-Deliver}}
\newcommand{\rmc}{\mbox{FR-MCast}}
\newcommand{\rmd}{\mbox{FR-Deliver}}
\newcommand{\optdel}[1]{\mbox{\opt({#1})}}
\newcommand{\consdel}[1]{\mbox{\cons({#1})}}
\newcommand{\rmcast}[2]{\mbox{\rmc({#1},{#2})}}
\newcommand{\rmdel}[1]{\mbox{\rmd({#1})}}


\title{A Quasi-Genuine FIFO Total Order Multicast Primitive}

%\author{
%xxx\\
%University \\ Country\\
%\and
%xxx\\
%University \\ Country\\
%\and
% ...
%}

\maketitle

\begin{abstract}


\end{abstract}

\section{Introduction}
\label{sec:intro}

Some multicast primitives have been devised in a such a way that multicast groups needed to communicate only when they had messages to exchange. These multicast primitives are called \emph{genuine}. We argue that it is possible, however, to devise a multicast primitive that, although not genuine, can make use of some knowledge given by the application to figure out which groups \emph{can} communicate with a given group. With such knowledge, although message exchanges take place even when there is no application message being transmitted between some two groups, such exchanges happen only when they are able to send to -- or receive from -- one another. The primitive that makes use of such property we call \emph{quasi-genuine}. 

\section{System model and definitions}
\label{sec:model}

We assume a system composed of nodes distributed over a geographical area. Nodes may fail by crashing and subsequently recover, but do not experience arbitrary behavior (i.e., no Byzantine failures). Communication is done by message passing, through the primitives \emph{send}$(p,m)$ and \emph{receive}$(m)$, where $p$ is the addressee of message $m$. Messages can be lost but not corrupted. If a message is repeatedly resubmitted to a \emph{correct node} (defined below), it is eventually received.

Our protocols ensure safety under both asynchronous and synchronous execution periods. The FLP impossibility result~\cite{fischer1985idc} states that under asynchronous assumptions consensus cannot be both safe and live. We thus assume that the system is initially asynchronous and eventually becomes synchronous. The time when the system becomes synchronous is called the \emph{Global Stabilization Time (GST)}~\cite{dwork1988cpp}, and it is unknown to the nodes.

Before GST, there are no bounds on the time it takes for messages to be transmitted and actions to be executed. After GST, such bounds exist but are unknown. After GST nodes are either \emph{correct} or \emph{faulty}. A correct node is operational ``forever" and can reliably exchange messages with other correct nodes. This assumption is only needed to prove liveness properties about the system. In practice, ``forever" means long enough for one instance of consensus to terminate.

%A game is composed of a set of objects. The game state is defined by the individual states of each one of its objects. We assume that the game objects are partitioned among different servers. Since objects have a location in the game, one way to perform this partitioning is by zoning the virtual world of the game. Each partition consists of a set of objects of the game and the server responsible for them is their \emph{coordinator}. As partitions represent physical regions in the game world, we define \emph{neighbor} regions as those which share a border with each other. We consider that each server is replicated, thus forming several groups which consist of the coordinator and its replicas. Therefore, for each region of the game world, there is a group assigned to it. This way, a group is said to coordinate an object when the group's coordinator is the coordinator for that object. Finally, groups are called neighbors when they are assigned to neighbor regions. From now on, the word `server' will be used for any kind of server, be it a coordinator or a replica.

%Each player may send his command to one of the servers -- which might not be the group's coordinator, if that provides a lower delay between the issuing of a command and its delivery. In the case of avatar based virtual environments, and as an avatar is usually also an object, the server to which a player is connected belongs to the group of his avatar's coordinator. A command $C = \{ c_1, c_2, ... \}$ is composed of one or more subcommands, one subcommand per object it affects. We refer to the set of objects affected by command $C$ as $obj(C)$. Also, we refer to the objects coordinated by a server $S$ as $obj(S)$. Finally, we define $obj(C,S)$ = \{$o$ : $o \in obj(C)$ and $o \in obj(S)$\}.

%Each object has its own command stream. When a command affects more than one object, it must be inserted in the command stream of all the objects it affects.

%Our consistency criterion is ``eventual linearizability". (I'm not sure this is indeed what we want and how to define it, but we do need some consistency criterion...)


%In order to guarantee that commands are executed properly by servers, theywe define the following order requirement.
% 
%Let $G =(V,E)$ be a graph where the set $V$ of vertices contains all game commands and $E$ contains edge $C_i \rightarrow C_j$ if subcommand $c_x \in C_i$ is executed before subcommand $c_y \in C_j$. The order requirement states that $G$ is acyclic. In other words, if two commands affect common objects, their subcommands must be executed in the same order.
%
%Our goal is to design ordering protocols with low latency. Two aspects affect latency: (a)~the number of communication steps needed to ensure that subcommands can be properly executed in the absence of contention and (b)~possible slowdowns due to contention. We illustrate these aspects in the following section.
%
%To each partition, a group of servers is assigned, and they are responsible for storing and managing the state of those objects. One of them is a coordinator (or region manager, if one considers partitions as regions) and the rest of the group is formed by replicas, whose purpose is to provide fault tolerance.



%\section{Protocol}

%To ensure reliability despite server crashes, each server is replicated using state-machine replication, implemented with consensus (e.g. Paxos \cite{lamport1998ptp}). Each player sends his commands to the server to which he is connected, which then proposes that command in a consensus instance. Each command is assigned a timestamp and executed against objects in timestamp order. We implement this by using a logical clock in each server group. Guaranteeing that the same set of commands is executed by the respective affected objects in the same order provides the level of consistency we are seeking. Therefore, the challenge is how to assign timestamps to commands such that consistency is not violated and commands are not discarded due to stale timestamp values.

%However, providing such level of consistency may prove to be costly in an MMOG context, since there may be several communication steps between the sending of a command and its atomic delivery. For this reason, we use a primitive we call quasi-genuine global fifo total order multicast, which is described in section \ref{sec:quasi}.

\section{Quasi-genuine Global FIFO Multicast}
\label{sec:quasi}

%To reduce the time needed to deliver a message, we use a multicast primitive which delivers messages optmistically, based on the time when they are created in a sender. This \emph{optimistic} delivery, although not guaranteeing that all replicas will receive all messages, is designed to have a fairly low latency, counting from when each message is sent until it is delivered.

%The final -- conservative, fault tolerant, but costly in terms of communication steps for each message -- delivery order should be as close as possible to the optimistic one, so that no rollbacks would be deemed necessary. To explain how this works, we first should understand the idea behind the optimistic delivery. Also, we must define what ``quasi-genuine'' and ``global FIFO'' means.

\textbf{Genuine} multicast protocols are those where two multicast groups only communicate with each other when one has some message to send to the other. A \textbf{quasi-genuine} multicast protocol assumes that: 

\begin{center}
\emph{A1: every group knows from which other groups a multicast message can possibly arrive, and to which other groups it can send a multicast message.}
\end{center}

In a quasi-genuine multicast protocol, different groups communicate with each other even if there is no message to be sent from one to the other, but, from A1, there is no need for a group $g_i$ to communicate with some other group $g_j$ which cannot send messages to $g_i$, or receive messages from $g_i$\footnote{This relation may even be asymmetric: it could be possible to send a message from group $g_i$ to $g_j$, but not in the opposite direction. In this case, in a protocol where a group is blocked waiting for possible messages from other groups, $g_j$ may block its delivery of messages waiting for some kind of ``clearance'' from $g_i$, but $g_i$ will never block waiting for messages from $g_j$.}. This information may be given by the application which is making use of such primitive, so, although not genuine, a quasi-genuine protocol does not imply that each group has to keep sending messages to every other group.

We define $sendersTo(G)$ as the set of groups which are able to send a message to $G$. Also, we define $receiversFrom(G)$ as the set of groups who are able to receive a message from $G$.

As messages are delivered according to their generation time, the delivery order is FIFO. As every two messages which are delivered in different groups should be delivered in the same order in these groups, we need total order. For the formal definition of the properties of the algorithm, assuming a crash-stop model, we have:

\textbf{Uniform Validity}: if a process \cms{}s $m$, then one of the correct processes that is a destination of $m$ eventually \cons{}s $m$.

\textbf{Uniform Agreement}: if a process \cons{}s $m$, then every correct process that is a destination of $m$ also \cons{}s $m$.

\textbf{Uniform Integrity}: for any message $m$, every correct process $p$ \cons{}s $m$ at most once, and only if some process executed \cmsend{$m$} and $p$ is one of $m$'s destinations.

\textbf{Uniform Total Order}: if processes $p$ and $p'$ both \cons\ $m$ and $m'$, then $p$ \cons{}s $m$ before $m'$ if and only if $p'$ \cons{}s $m$ before $m'$.

\textbf{FIFO Order}: if a correct process \cms{}s $m$ before it \cms{}s $m'$, then no correct process that is a destination of both $m$ and $m'$ \cons{}s $m'$, unless it has previously \cons{}ed  $m$.

Here, we consider that there are several processes. Each process $p$ belongs to a group $G = group(p)$, that is, $p \in G = group(p)$. 

%\subsubsection{Optimistic delivery}

%Even when ordering commands relevant to only one group, a significant number of communication steps is required: player to replica; replica to coordinator; consensus (two communication steps in the case of Paxos, when having the coordinator as the leader, using ballot reservation, each acceptor sending phase 2b messages to all other acceptors and assuming a tolerable number of failures) and, once consensus is achieved, each replica can put the command in the delivery queue and send it to each player. That sums up to five communication steps. On a geographically distributed scenario, this number of communication steps may be prohibitive for the ``playability'' of a vast number of online games.

%In order to mitigate this problem, we propose a protocol for \emph{optimistic ordering and delivery} of the commands, which is supposed to run in parallel with the protocol described in the previous section, which from now on will be called \emph{conservative ordering and delivery}. The optimistic order and delivery are correct if they include all the messages delivered with the conservative one and if they are delivered in the same order. Also, each object has an optimistic state and a conservative state. When a command is conservatively delivered to an object, its conservative state is updated, and this state is certainly consistent -- given the assumptions we have made about the system. Likewise, when a command is optimistically delivered, its optimistic state is updated.

%The optimistic ordering and delivery is supposed to be much faster than the conservative one. Assuming that it will be, and if the optimistic delivery order is always correct -- which means that it correctly predicted the conservative delivery order --, then the objects will always have a valid state within much fewer communication steps, counting from when each command $C$ was issued to when it is delivered and applied to $obj(C)$.

%The basic idea of the optimistic ordering is the following: assuming that the processes have a synchronized clock\footnote{We don't require here perfectly synchronized clocks, as the optimistic protocol tolerates mistakes by its very definition. We only need clocks which are synchronized enough, so that our delivery order prediction succeeds and matches the conservative delivery order.}, whenever a process $p$ receives a message $m$ from a client, it immediately applies a timestamp $ts$ to it, which consists simply of the current value of $p$'s wallclock, $now$. Therefore, $m.ts=now$. A wait window of length $w(p)$ is considered, where $w(p)$ is defined as the highest value of the estimated communication delay plus the wallclock deviation between the process $p$ and any of the other processes in its group $G$ or in any of the groups in $sendersTo(G)$.

%More formally, let $\delta(p',p)$ be the maximum time for a message from $p'$ to arrive at $p$. Also, let $\epsilon(p',p)$ be the deviation between the wallclocks of $p$ and $p'$. The value of $w(p)$ is estimated as the maximum value of \mbox{$(\delta(p',p) + \epsilon(p',p))$} for every $p'$ in $G = group(p)$ or in some group of $sendersTo(G)$.

%After applying the timestamp to $m$, the process $p$ immediately forwards it to all the other processes involved, including those in other groups. A process is involved with a message $m$ when it is one of its destinations, which can be inferred from $m.dst$. Then, $p$ puts $m$ in an \mbox{\textit{optPending}} list, where it stays until $now>m.ts+w(p)$, which means that $m$ has been in that list for a time longer than the defined wait window. In the meantime, other messages, sent from other processes, may have been received and also inserted in that list, always sorting by their timestamps.

%If $w(p)$ has been correctly estimated, and no message was lost, than all the messages that were supposed to be delivered before $m$ have necessarily been received already. If the same has occurred for all the processes, then all of them have received all the messages, and can deliver them in the same order of $ts$.

%To avoid out-of-order deliveries, if when $m$ arrives at $p$, \mbox{$m.ts < now - w(p)$}, wich means that $m$ arrived too late, the message $m$ is simply discarded by $p$, since another message $m' : m'.ts > m.ts$ may have already been delivered\footnote{We could make this in a way such that $m$ is only discarded by $p$ if, in fact, there was a delivered message $m' : m'.ts > m.ts$. If there was no such message, $m$ could still be delivered without violating the order we defined.}.

%However, even if the optimistic order is the same for all the processes, it won't be valid if the conservative order is different from it. For that reason, we devised a way to make the conservative delivery order as close as possible to the optimistic one.

\subsection{Message delivery}

%Instead of having the conservative delivery done completely independently from the optimistic one, we can actually use the latter as a hint for the final delivery order. Since the optimistic delivery should be fast when compared to the conservative one, waiting for it should not decrease the system performance significantly. Also, if we wait a short period longer, we can avoid a rollback later caused by mismatches between the two delivery orders, which is not desirable. The basic idea is, then, to pick the optimistic delivery order seen at the processes of each group.

%The complicating factor is the possibility of a command affecting the state of objects in more than one group. So, all involved groups must somehow agree regarding the delivery order of these commands. However, as in most MMOGs objects interact only with other objects which are nearby, we can exploit the fact that the groups addressed by a single command are usually neighbors. We do this by defining \emph{barriers} for multicast, such that $barrier(G_{send},G_{recv})$ = $t$ means that the group $G_{send}$ promised that no commands with timestamp lower than $t$ would be sent to group $G_{recv}$ anymore. When a server $S$ has received all the barrier values from its neighbours, and they are all greater than a value $t$, then $S$ knows that no more commands are coming from other groups and that, once the local ordering is done, all the commands with timestamp up to $t$ can be conservatively delivered. Besides, a barrier is sent along with the bundle of all messages with timestamp greater than the last previous barrier sent from $G_{send}$ to $G_{recv}$, so that when a server has received a barrier from a group, it means that it knows all the messages sent by that group until the time value stored in that barrier.

Each message $m$ has a source group $m.src$, a set of destination groups $m.dst$ and a timestamp $m.ts$. Note that, by abuse of notation, we have `process \mbox{$p \in m.dst$'} instead of `\mbox{$\exists$ group $G \in m.dst : \text{process }p \in G$'}. The total order delivery of messages in a group can be solved by using consensus. Each consensus instance agrees upon some message set as the next ones to be delivered -- messages within the same set are told apart by the timestamp $m.ts$ applied by the process which created them; to solve timestamp collisions, the unique id of the sender process can be used. If processes send messages to each other using FIFO reliable channels, and after receival, messages are proposed via consensus in the order in which they are received, the FIFO delivery order is ensured.

The complicating factor is the possibility of a message having at least one destination group different from its source group. So, all involved groups must somehow agree regarding the delivery order of these messages. However, from assumption A1, each group knows which other groups it could send messages to -- or receive messages from. We can use this by defining \emph{barriers} for multicast, such that $barrier(G_{send},G_{recv})$ = $t$ means that the group $G_{send}$ promised that it would send no more messages with a timestamp lower than $t$ to group $G_{recv}$. We have defined that $sendersTo(G) =$ \mbox{$\{G' : G'\text{ is able to send a message to }G\}$}. When a process $p$, from group $G$, has received all the barrier values from all the groups in $sendersTo(G)$, and they are all greater than a value $t$, then $p$ knows that no more messages with timestamp lower than $t$ are coming from other groups and that, once the local ordering (the ordering of messages originated in $G$) is done, all the messages with timestamp up to $t$ can be delivered. Besides, a barrier is sent along with the bundle of all messages with timestamp greater than the last previous barrier sent from $G_{send}$ to $G_{recv}$, so that when a process has received a barrier from a group, it means that it knows all the messages sent by that group until the time value stored in that barrier.

As mentioned before, we use consensus to deliver messages. Consider that each consensus instance $I$ from each group $I.grp$ receives a monotonically increasing unique integer identifier, without gaps, that is, for any two instances $I_i$ and $I_k$, such that $I_i.grp = I_k.grp$, if $I_i.id + 1 < I_k.id$, there is necessarily an instance $I_j : I_i.grp = I_j.grp = I_k.grp \wedge I_i.id < I_j.id < I_k.id$. No group runs two consensus instances in parallel: before initiating an instance of id $k+1$ each process checks whether the instance $k$ has already been decided, so some messages may wait to be proposed. When a process is allowed to initiate a new consensus instance, the pending messages may be proposed as a batch.%Consider also that each message $m$ sent by a group $G$ has a group sequence number $m.gs$ related to the order in which it is conservatively delivered, relatively to other messages also sent by $G$. As messages are conservatively delivered via consensus, the group sequence number of a message is equal to the id of the instance in which it was decided, that is, $m.gs = i \Leftrightarrow \exists I : I.id = i \wedge I.grp = m.src \wedge I.val = m$, where $I.val$ is the value decided by the instance $I$.

However, this implies using the timestamp given at the creation of a message by it sender. Therefore, it might be the case that, after a message $m'$ has been proposed by a process $p$ in a consensus instance, a message $m : m.ts < m'.ts$ arrives at $p$. If $m$ is delivered with its original timestamp, the timestamp order is violated and there is no sense in using these timestamps for barriers. There are two possible solutions for that: either the message is simply discarded, and no violation to the timestamp order takes place, or we can change the timestamp of $m$ to something greater than the timestamp of $m'$. We want to ensure uniform validity, so discarding $m$ is not an option. As we need, then, to change the value of $m.ts$, two things should be noted: as we are using reliable FIFO channels, then the process which sent $m$ is
different than that which sent $m'$, so inverting their order does not violate FIFO; finally, increasing a message timestamp must be done with caution, so that messages created by different groups at the same time have roughly the same timestamp and the barrier mechanism is efficient -- if a long sequence of messages have their timestamps increased, the last one of them may cause a group to wait a long time until it has all the barriers required to deliver it.

% To allow for the timestamp of messages to be increased and still have them delivered as soon as possible, their timestamps are increased by an infinitesimal value. For that reason, each timestamp value will consist of a real-time clock value, and a sequence value, which is used only when messages need to have their timestamps changed. Also, to solve timestamp clashes, the id of the process which sent the message is also used. Therefore, we have that $m.ts = (rtc, seq, p_{id})$, where $rtc$ is some value related to the wallclock (real-time clock) of a process, $seq$ is a sequence number to define an order between messages with the same $rtc$ value and $p_{id}$ is the id of the process which sent $m$. Therefore, we have:
% \begin{align*}
% m.ts < m.ts~&\Leftrightarrow~m.ts.rtc < m'.ts.rtc\\
% &\vee (m.ts.rtc = m'.ts.rtc \wedge m.ts.seq < m'.ts.seq)\\
% &\vee (m.ts.rtc = m'.ts.rtc \wedge m.ts.seq = m'.ts.seq \wedge m.ts.p_{id} < m'.ts.p_{id})
% \end{align*}

To allow for the timestamp of messages to be increased and still have them delivered as soon as possible, their timestamps are increased by an infinitesimal value. For that reason, each timestamp value will consist of a real-time clock value, and a sequence value, which is used only when messages need to have their timestamps changed. Therefore, we have that $m.ts = (rtc, seq)$, where $rtc$ is some value related to the wallclock (real-time clock) of a process and $seq$ is a sequence number to define an order between messages with the same $rtc$. Then we have:

\begin{align*}
m.ts < m'.ts \Leftrightarrow m.ts.rtc < m'.ts.rtc \vee (m.ts.rtc = m'.ts.rtc \wedge m.ts.seq < m'.ts.seq)\\
%&\vee (m.ts.rtc = m'.ts.rtc \wedge m.ts.seq = m'.ts.seq \wedge m.ts.p_{id} < m'.ts.p_{id})
\end{align*}

There are three possibilities for each message $m$, in the perspective of a process $p$ of $G$:

\begin{itemize}
  \item The message $m$ was originated in $G$, which is the only destination of $m$:
  
  In this case, when $m$ is received by $p$, $p$ checks whether the latest consensus instance $I_{k}$ in which it participated, or is trying to start, has already been decided -- if not, $p$ enqueues $m$ in a $propPending$ queue as the next message being proposed by it in the consensus instance $I_{k+1}$, so other tasks can keep being executed. Then, once $I_{k}$ has been decided, $p$ may start a new instance. Before that, all messages in $propPending$ that have been already decided are discarded from $propPending$. The rest is proposed as a batch in $I_{k+1}$. Once $m$ is decided, it is not immediately delivered to the application. Instead, $p$ checks whether some message $m_{prv} : m_{prv}.ts > m.ts$ has been decided previously. If that is the case, the value of $m.ts$ is changed to a value greater than the timestamp of any other message previously decided within $G$. Then, $m$ is inserted into a $barPending$ list for later being delivered, which will happen once every group $G'$ in $sendersTo(G)$ has already sent a message $barrier(G',G) = t$, such that $t > m.ts$. This is done because there could be a message $m'$ yet to come from another group $G'$, such that $m'.ts < m.ts$.

  \item When $m$ is originated in $G$, but it has at least one group other than $G$ as a destination:
  
  In this case, when $m$ is received, $p$ tries to initiate a consensus instance within $G$ to decide $m$. If $p$ cannot start the proposal now, $m$ is enqueued in $propPending$ for being proposed later along with other pending messages. Then, once $p$ may start a new instance, all messages in $propPending$ that have been already decided are discarded from $propPending$. The rest is proposed as a batch in the new consensus instance. Once any message $m$ is decided, $p$ checks whether some message $m_{prv} : m_{prv}.ts > m.ts$ has been decided previously. If that is the case, the value of $m.ts$ is changed to a value greater than the timestamp of any other message previously decided within $G$. Then, if $G \in m.dst$, $m$ is inserted in the $barPending$ list. Besides, when $m$ is decided, $p$ sends $m$ to every \mbox{$p' \in (m.dst \setminus \{G\})$}. When $m$ is received by each $p' \in G'$, $p'$ checks whether it has ever inserted $m$ in its own $barPending$ list. If not, $p'$ inserts $m$ into $barPending$ and adjusts $barrier(G, G')$ to $m.ts$. To ensure that, once a message $m$ is received from another group $G$, every message $m' : m'.ts<m.ts$ also from $G$ has already been received, every message is sent through a lossless FIFO channel\footnote{An ordinary TCP connection would be enough to provide such FIFO lossless channel.}.
  
  \item When $G$ is one of the destinations of $m$, but $m$ was originated in some other group $G'$:
  
  In this case, when $m$ is received for the first time\footnote{Multiple processes may have sent $m$. To ensure integrity and order, only the first delivery is considered.} by $p$, $m$ is inserted into the $barPending$ list of $p$ and the value of $barrier(G',G)$ is set to $m.ts$.
\end{itemize}

The messages in the \textit{barPending} list are always sorted in ascending order of their timestamps. When the first message $m$ in the $barPending$ list of a process $p \in G$ is such that $m.ts < barrier(G, G')$ for all $G' \in sendersTo(G)$, then $m$ is \cons{}ed by $p$ to the application as the next message. We claim that this delivery respects the FIFO total order\footnote{Proof needed.}.

%The possibility not covered here is when a message is sent from a group to another one which is not its neighbour. However, it is realistic for this context to assume that objects should be close to each other in the virtual environment to interact and that, as such, this kind of interaction is not supported. Nevertheless, one simple way to provide this kind of support could be by each group waiting for the barriers of every other group in the system, instead of only those of its neighbors.

A more formal description of the protocol is given in Algorithm \ref{algorithm:quasi}. We consider that three primitives are given: $getTime()$, which returns the current value of the local wallclock; $Propose(k, val)$, which proposes a value $val$ for the consensus instance of id $k$ within its group; and also $Decide(k, val)$, which is called when the consensus instance of id $k$ finishes. $Decide(k, val)$ is called for all the processes of the group that initiated it, when they learn that the value $val$ has been agreed upon in instance of id $k$. For the sake of simplicity, we assume that, for consensus instances within the same group, the values are decided in the same order of the instances id's\footnote{This can be easily done by delaying the callback of $Decide(k, val)$ while there is some unfinished consensus instance of id $k': k' < k$ from the same group.}. Finally, we also use a FIFO reliable multicast primitive \rmcast{$m$}{$groupSet$}, which \rmd{}s $m$ to all the processes in all the groups in $groupSet$ in one communication step, in FIFO order (e.g. the one described in \cite{ufrmcast1delta}).

Moreover, each process $p$ of group $G$ keeps some lists of messages:
\begin{itemize}
  %\item \textit{optPending}, which contains the messages waiting to be \mbox{\textit{\opt{}ed}} (optimistically delivered);
  \item \textit{propPending}, containing the messages waiting to be proposed by $p$;
  \item \textit{barPending}, with the messages ready to be \textit{\cons{}ed}, but which may be waiting for barriers from the groups in $sendersTo(G)$;
  \item \textit{decided}, which contains the messages that have already been proposed and decided within $group(p)$;
  \item \textit{delivered}, which contains the messages that have been \cons{}ed already.
\end{itemize} 

%; and $gs$, which is $m$'s sequence number relatively to its group of origin. 

Something that must be noticed is that each message $m$ might not have its source group as a destination. Anyway, $m$ still has to be agreed upon in its group of origin $G$, so that its order among other messages from $G$ may be decided and for %$m.gs$ can be decided, and also for 
$m$ to be retrievable even in the presence of failures.% Because of that, to ensure that messages of this kind are also received by $group(p)$'s coordinator $c$, which then initiates the Paxos instance that allows for the conservative delivery of the message in all of its destinations, every message $m$ from $p$ must also be sent to $c$ (l. \ref{algline:sendcoord}). This way, when the clock value at the coordinator $time(c) > w(c) + m.ts$, $c$ will conservatively send $m$ to all the processes in $m.dst$. Finally, to avoid delivering a message $m$ in a group it is not addressed to, before OPT-Delivering it, each process must check whether it is one of $m$'s destinations or not (l. \ref{algline:notinterested}). In l. \ref{algline:sendcoord}, $m$ is sent to the whole group $G$, so that in the case of failure of $G$'s coordinator, it is more likely that the new coordinator will know about the existence of $m$.

\begin{algorithm}
\begin{distribalgo}[1]

\blankline
\INDENT {Initialization}
  \STATE $k \leftarrow 0$, $nextProp \leftarrow 0$, $decided \leftarrow \emptyset$, $delivered \leftarrow \emptyset$, $propPending \leftarrow \emptyset$, $barPending \leftarrow \emptyset$
  \INDENT{\textbf{for all} $G' \in sendersTo(G)$ \textbf{do}}
    \STATE $barrier(G',G) \leftarrow -\infty$ 
  \ENDINDENT
\ENDINDENT 

\blankline
\INDENT{\textit{To} \cms{} \textit{a message $m$}}
  \STATE $m.ts \leftarrow (getTime(),0)$
  %\COMMENT{current wallclock value as the timestamp of $m$}
  \STATE \rmcast{$m$}{$\{G\}$}
%  \INDENT{\textbf{for all} $p' \in m.dst \cup \{G\}$ \textbf{do}}
%    \STATE send($p'$, $m$)    
%    \COMMENT{send optimistically $m$ to all involved processes}
%  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} \rmdel{$m'$}}
  \IF{$G = m'.src$}
    \STATE $propPending \leftarrow propPending \cup \{m'\}$
  \ELSIF{$m' \notin barPending \wedge m' \notin delivered$}
    \STATE $barPending \leftarrow barPending \cup \{m'\}$  
    \STATE $barrier(m'.src,G) \leftarrow m'.ts$ \label{algline:incbar}
  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in propPending \wedge nextProp = k$}}
%  \STATE $propPending \leftarrow propPending \setminus \{m\}$
%  \IF {$m \notin decided \wedge \nexists m' \in decided: m'.ts > m.ts$}
    \STATE $propPending \leftarrow propPending \setminus decided$%(decided \cup \{m' : \exists m'' \in decided \wedge m'.ts < m''.ts \wedge m' \neq null\})$ 
    \label{algline:nullstays}
    \IF {$propPending \neq \emptyset$}
      \STATE $nextProp \leftarrow k + 1$
      \STATE Propose($k$,$propPending$)
    \ENDIF
%  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When} Decide($k$,$msgSet$)}
%  \STATE $m.gs \leftarrow k$
  \INDENT{\textbf{while} $\exists m \in msgSet : (\forall m' \in msgSet : m \neq m' \Rightarrow m.ts < m'.ts)$ \textbf{do}}
    \STATE $msgSet \leftarrow msgSet \setminus \{m\}$
    \COMMENT{the messages are handled in ascending order of timestamp}
    \IF{$\exists m' \in decided : m'.ts \geq m.ts \wedge (\nexists m'' \in decided : m''.ts > m'.ts)$}
      \STATE $m.ts \leftarrow (m'.ts.rtc, m'.ts.seq + 1)$
    \ENDIF     
    \IF{$G \in m.dst$} \label{algline:checkcons}
      \STATE $barPending \leftarrow barPending \cup \{m\}$
    \ENDIF
    \STATE $decided \leftarrow decided \cup \{m\}$  
    \STATE \rmcast{$m$}{$m.dst \setminus \{G\}$} 
  \ENDINDENT
  \STATE $nextProp \leftarrow k + 1$
  \STATE $k \leftarrow k + 1$
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in barPending : \forall G' \in sendersTo(G): m.ts < barrier(G',G)$\\ $\wedge\ \nexists m' \in barPending : m'.ts < m.ts$ }}
  \STATE $barPending \leftarrow barPending \setminus \{m\}$
  \STATE \consdel{$m$}
  \STATE $delivered \leftarrow delivered \cup \{m\}$

\blankline
\ENDINDENT

\caption{\cmsend{m} -- executed by every process $p$ from group $G$}
\label{algorithm:quasi}
\end{distribalgo}
\end{algorithm}

% http://kdubezerra.googlecode.com/svn/trunk/textos/optpaxos/draftv3.pdf
% http://kdubezerra.googlecode.com/svn/trunk/textos/optpaxos/draftv3.tex
% 
% Eu tentei já fazer com que as instâncias de consenso fossem executadas em paralelo, mas é bem mais complicado do que eu esperava, e preferi deixar como está.
% 
% Pra exemplificar: como um processo, ao propor uma mensagem m, não sabe se m vai ser decidida, ele tem que mantê-la em uma fila pending, tal como no atomic broadcast do chandra, para ser proposta novamente se necessário.
% Pra evitar que ele fique propondo várias vezes a mesma mensagem, ainda mais outra lista é necessária (algo como trying), que guarda duplas (m, i), onde i é o id de uma instância de consenso.
% 
% Antes de um processo p propor uma mensagem m, ele verifica se m está em decided OU em trying. Se não estiver em nenhuma das duas, p propõe m numa instância de consenso Ia e a dupla (m,Ia) é inserida na lista trying. (m,Ia) é removido de trying quando Ia é resolvida. Se em alguma instância qualquer, m foi decidida, m é removida de pending e inserida em decided. p não pode propor novamente m enquanto Ia não for resolvida.
% 
% O resultado disso é que se, após o término de Ia, m ainda não estiver em decided, m é novamente proposta por p em uma instância Ib e (m,Ib) é inserida em trying. E assim por diante. 

Assuming $\delta$ as the communication delay between every pair of processes, the time needed to decide a message $m$ after it has been \cms{} by some process is equal, in the worst case, to $\delta + 2\tconsm$, where $\delta$ is the time needed to \rmd{} a message and \tcons\ is the time needed to execute a consensus instance. This value is counted twice because $m$ may have been inserted into $propPending$ right after a consensus instance has been initiated. In that case, $m$ would have to wait such consensus to finish, to be then proposed and finally decided. However, it may also happen that $m$ has been inserted into $propPending$ right before some proposal has been made, so it would take only $\delta + \tconsm$ to decide $m$. As $m$ may go into $propPending$ anytime between the worst and the best case with the same probability, the average time needed for deciding $m$ would be equal to $\delta + 1.5\tconsm$. Nevertheless, the time needed to finally \cons{} $m$ will depend on when barriers are received from other groups.

\subsubsection{Addressing liveness}

The problem with Algorithm \ref{algorithm:quasi} is that it does not guarantee liveness when a group has no message to receive from some other group and then keeps waiting for a new message to increase the barrier value and proceed with the delivery of new messages. However, liveness can be easily provided by sending periodic empty messages from $G$ to each $G' \in receiversFrom(G)$ to which no message has been sent for a specified time threshold \textit{barrierThreshold}. Algorithm \ref{algorithm:liveness} describes this. When a message $m$ from group $G$ to some other group $G'$ has been \rmc{} by $p \in G$, $p$ knows that the other processes of $G$ did the same and that $m$ will be eventually received by the processes of $G'$, serving as barrier from $G$ to $G'$ (l. \ref{algline:incbar} of Algorithm \ref{algorithm:quasi}). However, when there is a long period after the last time when such kind of message has been created, $p$ decides to create some empty message to send to the processes of $G'$ with the sole purpose of increasing their barrier values and allow for the delivery of possibly blocked messages in $G'$.

\begin{algorithm}
\begin{distribalgo}[1]
\blankline
\INDENT {Initialization}
  \INDENT{\textbf{for all} $G' \in receiversFrom(G)$ \textbf{do}}
    \STATE $lastBarrierCreated(G') = -\infty$
  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} \rmc{}ing a message $m$ to some group $G' \neq G$}
  \STATE $lastBarrierCreated(G') \leftarrow m.ts.rtc$
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists G' \in receiversFrom(G): getTime() - lastBarrierCreated(G') > barrierThreshold$}}
  \STATE $null \leftarrow$ empty message
  \STATE $null.ts \leftarrow (lastBarrierCreated(G') + barrierThreshold, 0)$ \label{algline:samenullid}
  \STATE $null.src \leftarrow G$
  \STATE $null.dst \leftarrow \{G'\}$ 
  \STATE $propPending \leftarrow propPending \cup \{null\}$
  \COMMENT{saving that nothing was sent until null.ts}
\ENDINDENT 

\blankline
\caption{Achieving liveness by sending periodic messages; executed by every process $p$ of group $G$}
\label{algorithm:liveness}
\end{distribalgo} 
\end{algorithm}

The problem with addressing liveness this way is that, in the worst case, $G'$ has decided a message $m$ and has just received the last barrier $b$ from $G$, such that $b < m.ts$. This would mean that, if $G$ has no messages to send to $G'$, $G'$ will have to wait, at least, for $barrierThreshold$ -- maybe just to receive some $b' : b' < m.ts$, having to wait again and so on. How long exactly it will take to \cons{} $m$ depends on many variables, such as how far in the past $m$ was created and how long it takes for some barrier $b: b > m.ts$ to arrive.% Because of that, the time between \cms{}ing and \cons{}ing $m$ be, in the worst case, $2w(p) + 4\tconsm + barrierThreshold$, where $w(p)$ is the wait window of the process $p \in G$, and \tcons\ is counted twice because there are two consensus proposals -- first in $G'$ regarding $m$, and then in $G$ regarding the empty message.

It is necessary to guarantee that a $null$ message will eventually be proposed, decided, and a barrier will be sent to some group which might be needing it, so that progression is guaranteed. Therefore, this kind of messages are created by every process in the group, since if any one of them does not have it, such message might never be decided. %Besides, they are excluded from the $propPending$ list only when decided in the group (l. \ref{algline:nullstays} of Algorithm \ref{algorithm:quasi}). Since they are obviously never delivered to the application, they can remain in such list and be decided even after some message with higher timestamp is decided. Not removing them unless they are decided ensures that they will be sent and that other groups will be able to increase their barrier values. % On the other hand, this brings another problem: if every process in $G$ creates a different $null$ message, many $null$ messages might have to be decided before a meaningful message.
To prevent the multiple $null$ messages -- created by different processes within the group -- of being decided, they could be created in a way such that the different processes can somehow figure out that two different $null$ messages are equivalent\footnote{This could be done by assuming no timestamp collisions and by using them to uniquely identify messages. Then, only one of the messages created with the same timestamp (l. \ref{algline:samenullid} of Algorithm \ref{algorithm:liveness}) would be decided.}.

However, there is a way to provide liveness withouth such a possibly long delay for the conservative delivery of messages, although that would imply creating more messages and making deeper changes in the delivery algorithm. Let $blockers(m)$ be defined as the set of groups whose barrier is needed in order for some group to deliver $m$. More formally, $blockers(m) =$ \mbox{$\{G_B : \exists G_{dst} \in m.dst \wedge G_B \in sendersTo(G_{dst})\}$}. The idea is that, once each group $G_B$ in $blockers(m)$ have sent a barrier $b > m.ts$ to all the groups belonging to \mbox{$m.dst \cap receiversFrom(G_B)$}, all possible destinations of $m$ can deliver it. This way, instead of relying on periodic messages, whenever a process $p$ in a group $G$ has a message $m$ to send, $p$ sends $m$ to every $p' \in m.dst \cup G \cup blockers(m)$. It is sent to the groups in $blockers(m)$, so that they know that there is a message which will be blocked until they send a proper barrier to unblock it.

%When the process $p'$ of some group $G'$ receives $m$, such that $m.src \neq G'$, $p'$ knows that there might be other groups depeding on the barrier of $G'$ to deliver $m$. For that reason, it will immediately create a $null$ message with a timestamp equal to $m.ts$ and with $null.dst = m.dst$ $\cap$ $receiversFrom(G')$. Then, $p'$ will insert such message in its $optPending$ queue -- to ensure that, once it goes to $propPending$, any message \mbox{$m': m'.ts < null.ts \wedge m'.src = G'$} is already there. As soon as $null.ts > now - w(p')$, the $null$ message will be proposed in $G'$ and, once $null$ is decided, each process of $G'$ will send a $\{null, \text{`cons'}\}$ message to each process in each group \mbox{$G \in m.dst \cap receiversFrom(G')$}. This way, any group which was waiting for a barrier from $G'$ to deliver $m$ will be able to do so as soon as it receives such $null$ message. The new delivery algorithm would be as described in \mbox{Algorithm {\ref{algorithm:ondemand}}}.
% 
% \begin{algorithm}
% \begin{distribalgo}[1]
% 
% \blankline
% \INDENT {Initialization}
%   \STATE $k \leftarrow 0$, $nextProp \leftarrow 0$, $decided \leftarrow \emptyset$, $propPending \leftarrow \emptyset$, $optPending \leftarrow \emptyset$, $barPending \leftarrow \emptyset$
%   \INDENT{\textbf{for all} $G' \in sendersTo(G)$ \textbf{do}}
%     \STATE $barrier(G',G) \leftarrow -\infty$ 
%   \ENDINDENT
% \ENDINDENT 
% 
% \blankline
% \INDENT{\textit{To send a message $m$} -- \cmsend{m}}
%   \STATE $m.ts \leftarrow getTime()$  
%   \COMMENT{current wallclock value as the timestamp of $m$}
%   \STATE \rmcast{$m$}{$m.dst \cup \{G\} \cup blockers(m)$}
% %  \INDENT{\textbf{for all} $p' \in m.dst \cup \{G\} \cup blockers(m)$ \textbf{do}}
% %    \STATE send($p'$, $m$)    
% %    \COMMENT{send optimistically $m$ to all involved processes}
% %  \ENDINDENT
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When} \rmdel{$m'$}}
%   \IF {$G \neq m'.src$ $\wedge$ $\exists G' \in (m'.dst \cap receiversFrom(G))$}
%     \STATE $null \leftarrow$ empty message
%     \STATE $null.src \leftarrow G$  \COMMENT{some other group needs a barrier from this one}
%     \STATE $null.ts \leftarrow m'.ts$ \label{algline:nulltsmts}
%     \STATE $null.dst \leftarrow m'.dst \cap receiversFrom(G)$
%     \STATE $optPending \leftarrow optPending \cup \{null\}$
%   \ENDIF
%   \IF {$m'.ts < getTime() - w(p) \vee G \notin m'.dst$}
%     \STATE discard $m'$
%     \COMMENT{late commands probably lead to out-of-order delivery}
%   \ELSE
%     \STATE $optPending \leftarrow optPending$ $\cup$ $\{m'\}$
%   \ENDIF
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When $\exists m \in optPending : getTime() > m.ts + w(p)$ $\wedge\ \nexists m' \in optPending: m'.ts < m.ts$}}
%   \STATE $optPending \leftarrow optPending \setminus \{m\}$
%   \IF {$G \in m.dst \wedge m \neq null$}
%     \STATE \optdel{$m$}  
%   \ENDIF
%   \IF {$G = m.src$}
%     \STATE $propPending \leftarrow propPending \cup \{m\}$
%   \ENDIF
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When $\exists m \in propPending \wedge nextProp = k$}}
% %  \STATE $propPending \leftarrow propPending \setminus \{m\}$
% %  \IF {$m \notin decided \wedge \nexists m' \in decided: m'.ts > m.ts$}
%     \STATE $propPending \leftarrow propPending \setminus (decided \cup \{m' : \exists m'' \in decided \wedge m'.ts < m''.ts \wedge m' \neq null\})$ \label{algline:keepnull}
%     \IF {$propPending \neq \emptyset$}
%       \STATE $nextProp \leftarrow k + 1$
%       \STATE Propose($k$,$propPending$)
%     \ENDIF
% %  \ENDIF
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When} Decide($k$,$msgSet$)}
% %  \STATE $m.gs \leftarrow k$
%   \STATE $decided \leftarrow decided \cup msgSet$
%   \INDENT{\textbf{for all} $m \in msgSet : G \in m.dst \wedge m \neq null$ \textbf{do}} \label{algline:checkcons}
%     \STATE $barPending \leftarrow barPending \cup \{m\}$
%   \ENDINDENT
%   \INDENT{\textbf{while} $\exists m \in msgSet : (\forall m' \in msgSet : m \neq m' \Rightarrow m.ts < m'.ts)$ \textbf{do}}
%     \INDENT{\textbf{for all} $p' \in m.dst \setminus \{G\}$ \textbf{do}}
%       \STATE send($p'$, $\{m, \text{`cons'}\}$)
%       \COMMENT{this message is sent through a FIFO lossless channel}
%     \ENDINDENT
%     \STATE $msgSet \leftarrow msgSet \setminus \{m\}$
%     \COMMENT{the messages are sent in ascending order of timestamp}
%   \ENDINDENT
%   \STATE $nextProp \leftarrow k + 1$
%   \STATE $k \leftarrow k + 1$  
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When} receive($\{m',\{\text{`cons'}\}$)}  
%   \IF {$m' \notin\ barPending \wedge m' \notin\ delivered \wedge m' \neq\ null$}
%     \STATE $barPending \leftarrow barPending \cup \{m\}$
%   \ENDIF
%   \STATE $barrier(m'.src,G) \leftarrow max(m'.ts, barrier(m'.src,G))$  \COMMENT{channels are FIFO, but there are different senders}
% \ENDINDENT
% 
% \blankline
% \INDENT{\textit{When $\exists m \in barPending : \forall G' \in sendersTo(G): m.ts < barrier(G',G)$\\ $\wedge\ \nexists m' \in barPending : m'.ts < m.ts$ }}
%   \STATE $barPending \leftarrow barPending \setminus \{m\}$
%   \STATE \consdel{$m$}
%   \STATE $delivered \leftarrow delivered \cup \{m\}$
% 
% \blankline
% \ENDINDENT
% 
% \caption{\cmsend{m} -- executed by every process $p$ from group $G$}
% \label{algorithm:ondemand}
% \end{distribalgo}
% \end{algorithm}
% 
% Now, assuming that $w(p)$ is the same for every process $p$, we have a worst case delivery latency for $m$ of $w(p) + 2\tconsm$, as the $null$ message is created and proposed in parallel with $m$. However, each sender process is sending $m$ not only to every destination process, but also to every process in its group and to every process belonging to some group of $blockers(m)$. This can create a fairly high amount of messages. Unfortunately, this has to be done to guarantee that such $null$ message will eventually be proposed, decided, and a barrier will be sent to some group which might be needing it. Although this kind of messages are excluded from the $propPending$ list only when decided in the group (l. \ref{algline:keepnull} of Algorithm \ref{algorithm:ondemand}), if at least one of the processes does not have it, it could never be decided. Again, like in Algorithm \ref{algorithm:liveness}, there might be many processes in a group proposing a $null$ message because of the same $m$. Although this is not wrong, it is inefficient, and a way to identify $null$ messages created for the same purpose should be devised\footnote{Again, we could use timestamps as unique identifiers and assume no timestamp collisions. But for that to work, instead of making $null.ts = m.ts$ (l. \ref{algline:nulltsmts} of Algorithm \ref{algorithm:ondemand}), we could define $null.ts$ as a little bit greater than $m.ts$.}.
% 
% % Nevertheless, one possible way to implement the delivery for a command $C$ of this type would be to initiate a Paxos consensus instance in each group addressed by it, proposing $C$ and having as learns all servers in all involved groups. 
% 
% %In brief, our strategy is to combine state-machine replication (implemented with Paxos), used to handle object replicas, with Skeen's algorithm, used to select timestamps for commands that affect multiple objects.
% 
% \subsection{Recovering from mistakes}
% 
% Unfortunately, even with a very good delay estimation (e.g. on an environment with a low jitter), there is absolutely no guarantee that the multicast protocol described  in section \ref{sec:quasi} will deliver the game command messages optimistically and conservatively in the same order. When it doesn't, it is considered a \emph{mistake}. Every mistake of the optimistic delivery -- either a lost command message, or an out-of-order delivery -- will cause a rollback of the optimistic state of the objects and re-execution of some of the optimistically delivered commands.
% 
% To perform that, we consider that each object has an optimistic delivery queue, $Q_{opt}$. Whenever a command is optimistically delivered, the optimistic state is updated and the command is pushed in the back of $Q_{opt}$. Whenever a command $C_c$ is conservatively delivered, it updates the conservative state of each object in $obj(C_c)$ and, for each one of them, the algorithm checks whether it is the first command in $Q_{opt}$. If it is, $C_c$ is simply removed from $Q_{opt}$ and the execution continues. If it isn't, it means that $C_c$ was either optimistically delivered out of order, or it was simply never optmistically delivered. It then checks whether $Q_{opt}$ contains $C_c$. If it does, it means the command was optimistically deliverd out of order, and it is removed from the list -- if $Q_{opt}$ doesn't contain $C_c$, it was probably lost\footnote{Also, when $C_c$ is delivered, but it is not in $Q_{opt}$, the remaining possibility is the very unlikely case where the conservative delivery happened before the optimistic one. To handle this case, $C_c$ is stored in a list of possibly delayed optimistic delivery and, if it is ever optimistically delivered, the algorithm will know that it should only discard that command, instead of updating the optimistic state.}. Then, the optimistic state is overwritten with the conservative one and, from that state, all the remaining comands in $Q_{opt}$ are re-executed, leading to a new optimistic state for that object.
% 

%\bibliographystyle{latex8}
%\bibliography{main}
\bibliographystyle{acm}
\bibliography{gftommog}
\end{document}

