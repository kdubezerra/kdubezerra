\documentclass[times, 10pt]{article} 
%\documentclass[times, 10pt,twocolumn]{article} 
%\usepackage[noend]{distribalgo}
\usepackage{algorithm}
%\usepackage{times}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amssymb}
\usepackage[noend]{distribalgo}
\usepackage[draft]{fixme}
\usepackage[hmargin=2cm,vmargin=2cm]{geometry}


\begin{document}

\newcommand{\mv}[1]{\ensuremath{\operatorname{\mathit{#1}}}}
\definecolor{dark}{gray}{.6}
\newcommand{\bc}[1]{\textcolor{dark}{#1}}
\newtheorem{lems}{Lemma}
\newtheorem{props}{Proposition}
\newtheorem{thms}{Theorem}
\newtheorem{defs}{Definition}
\newtheorem{obs}{Observation}

\newcommand{\code}[1]{\texttt{\small{\textbf{#1}}}}

\newcommand{\blankline}{\vspace{4 mm}}
\newcommand{\cmsend}[1]{GFTO-Mcast({#1})}
\newcommand{\tconsm}{T_{cons}}
\newcommand{\tcons}{$\tconsm$}



\title{Practical consistency management for geographically distributed MMOG servers}

%\author{
%xxx\\
%University \\ Country\\
%\and
%xxx\\
%University \\ Country\\
%\and
% ...
%}

\maketitle

\begin{abstract}


\end{abstract}

\section{Introduction}
\label{sec:intro}


\section{System model and definitions}
\label{sec:model}

We assume a system composed of nodes, divided into \emph{players} and \emph{servers}, distributed over a geographical area. Nodes may fail by crashing and subsequently recover, but do not experience arbitrary behavior (i.e., no Byzantine failures). Communication is done by message passing, through the primitives \emph{send}$(p,m)$ and \emph{receive}$(m)$, where $p$ is the addressee of message $m$. Messages can be lost but not corrupted. If a message is repeatedly resubmitted to a \emph{correct node} (defined below), it is eventually received.

Our protocols ensure safety under both asynchronous and synchronous execution periods. The FLP impossibility result~\cite{FLP85} states that under asynchronous assumptions consensus cannot be both safe and live. We thus assume that the system is initially asynchronous and eventually becomes synchronous. The time when the system becomes synchronous is called the \emph{Global Stabilization Time (GST)}~\cite{DLS88}, and it is unknown to the nodes.
Before GST, there are no bounds on the time it takes for messages to be transmitted and actions to be executed. After GST, such bounds exist but are unknown. After GST nodes are either \emph{correct} or \emph{faulty}. A correct node is operational ``forever" and can reliably exchange messages with other correct nodes. This assumption is only needed to prove liveness properties about the system. In practice, ``forever" means long enough for one instance of consensus to terminate.

A game is composed of a set of objects. The game state is defined by the individual states of each one of its objects. We assume that the game objects are partitioned among different servers. Since objects have a location in the game, one way to perform this partitioning is by zoning the virtual world of the game. Each partition consists of a set of objects of the game and the server responsible for them is their \emph{coordinator}. As partitions represent physical regions in the game world, we define \emph{neighbor} regions as those which share a border with each other. We consider that each server is replicated, thus forming several groups which consist of the coordinator and its replicas. Therefore, for each region of the game world, there is a group assigned to it. This way, a group is said to coordinate an object when the group's coordinator is the coordinator for that object. Finally, groups are called neighbors when they are assigned to neighbor regions. From now on, the word `server' will be used for any kind of server, be it a coordinator or a replica.

Each player may send his command to one of the servers -- which might not be the group's coordinator, if that provides a lower delay between the issuing of a command and its delivery. In the case of avatar based virtual environments, and as an avatar is usually also an object, the server to which a player is connected belongs to the group of his avatar's coordinator. A command $C = \{ c_1, c_2, ... \}$ is composed of one or more subcommands, one subcommand per object it affects. We refer to the set of objects affected by command $C$ as $obj(C)$. Also, we refer to the objects coordinated by a server $S$ as $obj(S)$. Finally, we define $obj(C,S)$ = \{$o$ : $o \in obj(C)$ and $o \in obj(S)$\}.

%Each object has its own command stream. When a command affects more than one object, it must be inserted in the command stream of all the objects it affects.

Our consistency criterion is ``eventual linearizability". (I'm not sure this is indeed what we want and how to define it, but we do need some consistency criterion...)


%In order to guarantee that commands are executed properly by servers, theywe define the following order requirement.
% 
%Let $G =(V,E)$ be a graph where the set $V$ of vertices contains all game commands and $E$ contains edge $C_i \rightarrow C_j$ if subcommand $c_x \in C_i$ is executed before subcommand $c_y \in C_j$. The order requirement states that $G$ is acyclic. In other words, if two commands affect common objects, their subcommands must be executed in the same order.
%
%Our goal is to design ordering protocols with low latency. Two aspects affect latency: (a)~the number of communication steps needed to ensure that subcommands can be properly executed in the absence of contention and (b)~possible slowdowns due to contention. We illustrate these aspects in the following section.
%
%To each partition, a group of servers is assigned, and they are responsible for storing and managing the state of those objects. One of them is a coordinator (or region manager, if one considers partitions as regions) and the rest of the group is formed by replicas, whose purpose is to provide fault tolerance.



\section{Protocol}

To ensure reliability despite server crashes, each server is replicated using state-machine replication, implemented with consensus (e.g. Paxos \cite{lamport1998ptp}). Each player sends his commands to the server to which he is connected, which then proposes that command in a consensus instance. Each command is assigned a timestamp and executed against objects in timestamp order. We implement this by using a logical clock in each server group. Guaranteeing that the same set of commands is executed by the respective affected objects in the same order provides the level of consistency we are seeking. Therefore, the challenge is how to assign timestamps to commands such that consistency is not violated and commands are not discarded due to stale timestamp values.

However, providing such level of consistency may prove to be costly in an MMOG context, since there may be several communication steps between the sending of a command and its atomic delivery. For this reason, we use a primitive we call quasi-genuine global fifo total order multicast, which is described in section \ref{sec:quasi}.

\subsection{Quasi-genuine Global FIFO Total Order Multicast}
\label{sec:quasi}

To reduce the time needed to deliver a message, we use a multicast primitive which delivers messages optmistically, based on the time when they are created in a sender. This \emph{optimistic} delivery, although not guaranteeing that all replicas will receive all messages, is designed to have a fairly low latency, counting from when each message is sent until it is delivered.

The final -- conservative, fault tolerant, but costly in terms of communication steps for each message -- delivery order should be as close as possible to the optimistic one, so that no rollbacks would be deemed necessary. To explain how this works, we first should understand the idea behind the optimistic delivery. Also, we must define what ``quasi-genuine'' and ``global FIFO'' means.

\textbf{Genuine} multicast protocols are those where two multicast groups only communicate with each other when one has some message to send to the other. A \textbf{quasi-genuine} multicast protocol assumes that: 

\begin{center}
\emph{A1: every group knows from which other groups a multicast message can possibly arrive, and to which other groups it can send a multicast message.}
\end{center}

In a quasi-genuine multicast protocol, different groups communicate with each other even if there is no message to be sent from one to the other, but, from A1, there is no need for a group $g_i$ to communicate with some other group $g_j$ which cannot send messages to $g_i$, or receive messages from $g_i$\footnote{This relation may even be asymmetric: it could be possible to send a message from group $g_i$ to $g_j$, but not in the opposite direction. In this case, in a protocol where a group is blocked waiting for possible messages from other groups, $g_j$ may block its delivery of messages waiting for some kind of ``clearance'' from $g_i$, but $g_i$ will never block waiting for messages from $g_j$.}. This information may be given by the application which is making use of such primitive, so, although not genuine, a quasi-genuine protocol does not imply that each group has to keep sending messages to every other group.

We define $sendersTo(G)$ as the set of groups which are able to send a message to $G$. Also, we define $receiversFrom(G)$ as the set of groups who are able to receive a message from $G$.

As messages are delivered according to their generation time, the delivery order is FIFO. Besides, as every two messages which are delivered in different groups should be delivered in the same order in these groups, we need total order. However, FIFO total order is not enough to guarantee one property that we need: let the send time $st(m)$ of a message $m$ be the value of the wallclock of $p$ when $m$ was sent, where $p$ is the process which sent it. We need that, if $st(m_1) < st(m_2)$, then $m_1$ should be delivered before $m_2$, if $m_1$ and $m_2$ are delivered by the same process. Although this property may be hard to observe, it is used by the optimistic delivery protocol, so that messages can be delivered in order in one communication step, in the best case. If, when each process sends a message $m$, it stamps the message with the current value $st(m)$ of its wallclock, we define:

\textbf{Global FIFO order}: if processes $p_i$ and $p_j$ send respectively $m_i$ and $m_j$ to the same destination $p_d$, then if $st(m_i) < st(m_j)$, $m_i$ is not delivered after $m_j$ in $p_d$.

Here, we consider that there are several processes. Each process $p$ belongs to a group $G = group(p)$, that is, $p \in G = group(p)$.% Each group $G$ has a special leader process $c$, which is its coordinator. 

\subsubsection{Optimistic delivery}

%Even when ordering commands relevant to only one group, a significant number of communication steps is required: player to replica; replica to coordinator; consensus (two communication steps in the case of Paxos, when having the coordinator as the leader, using ballot reservation, each acceptor sending phase 2b messages to all other acceptors and assuming a tolerable number of failures) and, once consensus is achieved, each replica can put the command in the delivery queue and send it to each player. That sums up to five communication steps. On a geographically distributed scenario, this number of communication steps may be prohibitive for the ``playability'' of a vast number of online games.

%In order to mitigate this problem, we propose a protocol for \emph{optimistic ordering and delivery} of the commands, which is supposed to run in parallel with the protocol described in the previous section, which from now on will be called \emph{conservative ordering and delivery}. The optimistic order and delivery are correct if they include all the messages delivered with the conservative one and if they are delivered in the same order. Also, each object has an optimistic state and a conservative state. When a command is conservatively delivered to an object, its conservative state is updated, and this state is certainly consistent -- given the assumptions we have made about the system. Likewise, when a command is optimistically delivered, its optimistic state is updated.

%The optimistic ordering and delivery is supposed to be much faster than the conservative one. Assuming that it will be, and if the optimistic delivery order is always correct -- which means that it correctly predicted the conservative delivery order --, then the objects will always have a valid state within much fewer communication steps, counting from when each command $C$ was issued to when it is delivered and applied to $obj(C)$.

The basic idea of the optimistic ordering is the following: assuming that the processes have a synchronized clock\footnote{We don't require here perfectly synchronized clocks, as the optimistic protocol tolerates mistakes by its very definition. We only need clocks which are synchronized enough, so that our delivery order prediction succeeds and matches the conservative delivery order.}, whenever a process $p$ receives a message $m$ from a client, it immediately applies a timestamp $ts$ to it, which consists simply of the current value of $p$'s wallclock, $now$. Therefore, $m.ts=now$. A wait window of length $w(p)$ is considered, where $w(p)$ is defined as the highest value of the estimated communication delay plus the wallclock deviation between the process $p$ and any of the other processes in its group $G$ or in any of the groups in $sendersTo(G)$.

More formally, let $\delta(p',p)$ be the maximum time for a message from $p'$ to arrive at $p$. Also, let $\epsilon(p',p)$ be the deviation between the wallclocks of $p$ and $p'$. The value of $w(p)$ is estimated as the maximum value of \mbox{$(\delta(p',p) + \epsilon(p',p))$} for every $p'$ in $G = group(p)$ or in some group of $sendersTo(G)$.

After applying the timestamp to $m$, the process $p$ immediately forwards it to all the other processes involved, including those in other groups. A process is involved with a message $m$ when it is one of its destinations, which can be inferred from $m.dst$. Then, $p$ puts $m$ in an \mbox{\textit{optPending}} list, where it stays until $now>m.ts+w(p)$, which means that $m$ has been in that list for a time longer than the defined wait window. In the meantime, other messages, sent from other processes, may have been received and also inserted in that list, always sorting by their timestamps.

If $w(p)$ has been correctly estimated, and no message was lost, than all the messages that were supposed to be delivered before $m$ have necessarily been received already. If the same has occurred for all the processes, then all of them have received all the messages, and can deliver them in the same order of $ts$.

To avoid out-of-order deliveries, if when $m$ arrives at $p$, \mbox{$m.ts < now - w(p)$}, wich means that $m$ arrived too late, the message $m$ is simply discarded by $p$, since another message $m' : m'.ts > m.ts$ may have already been delivered\footnote{We could make this in a way such that $m$ is only discarded by $p$ if, in fact, there was a delivered message $m' : m'.ts > m.ts$. If there was no such message, $m$ could still be delivered without violating the order we defined.}.

However, even if the optimistic order is the same for all the processes, it won't be valid if the conservative order is different from it. For that reason, we devised a way to make the conservative delivery order as close as possible to the optimistic one.

\subsubsection{Conservative delivery}

Instead of having the conservative delivery done completely independently from the optimistic one, we can actually use the latter as a hint for the final delivery order. Since the optimistic delivery should be fast when compared to the conservative one, waiting for it should not decrease the system performance significantly. Also, if we wait a short period longer, we can avoid a rollback later caused by mismatches between the two delivery orders, which is not desirable. The basic idea is, then, to pick the optimistic delivery order seen at the processes of each group.

%The complicating factor is the possibility of a command affecting the state of objects in more than one group. So, all involved groups must somehow agree regarding the delivery order of these commands. However, as in most MMOGs objects interact only with other objects which are nearby, we can exploit the fact that the groups addressed by a single command are usually neighbors. We do this by defining \emph{barriers} for multicast, such that $barrier(G_{send},G_{recv})$ = $t$ means that the group $G_{send}$ promised that no commands with timestamp lower than $t$ would be sent to group $G_{recv}$ anymore. When a server $S$ has received all the barrier values from its neighbours, and they are all greater than a value $t$, then $S$ knows that no more commands are coming from other groups and that, once the local ordering is done, all the commands with timestamp up to $t$ can be conservatively delivered. Besides, a barrier is sent along with the bundle of all messages with timestamp greater than the last previous barrier sent from $G_{send}$ to $G_{recv}$, so that when a server has received a barrier from a group, it means that it knows all the messages sent by that group until the time value stored in that barrier.

The complicating factor is the possibility of a message having at least one destination group different from its source group. So, all involved groups must somehow agree regarding the delivery order of these messages. However, from assumption A1, each group knows which other groups it could send messages to -- or receive messages from. We can use this by defining \emph{barriers} for multicast, such that $barrier(G_{send},G_{recv})$ = $t$ means that the group $G_{send}$ promised that it would send no more messages with a timestamp lower than $t$ to group $G_{recv}$. We have defined that $sendersTo(G) =$ \mbox{$\{G' : G'\text{ is able to send a message to }G\}$}. When a process $p$, from group $G$, has received all the barrier values from all the groups in $sendersTo(G)$, and they are all greater than a value $t$, then $p$ knows that no more messages with timestamp lower than $t$ are coming from other groups and that, once the local ordering (the ordering of messages originated in $G$) is done, all the messages with timestamp up to $t$ can be conservatively delivered. Besides, a barrier is sent along with the bundle of all messages with timestamp greater than the last previous barrier sent from $G_{send}$ to $G_{recv}$, so that when a process has received a barrier from a group, it means that it knows all the messages sent by that group until the time value stored in that barrier. 

We use consensus to conservatively deliver each message. Consider that each consensus instance $I$ from each group $I.grp$ receives a monotonically increasing unique integer identifier, without gaps, that is, for any two instances $I_i$ and $I_k$, such that $I_i.grp = I_k.grp$, if $I_i.id + 1 < I_k.id$, there is necessarily an instance $I_j : I_i.grp = I_j.grp = I_k.grp \wedge I_i.id < I_j.id < I_k.id$. Consider also that each message $m$ sent by a group $G$ has a group sequence number $m.gs$ related to the order in which it is conservatively delivered, relatively to other messages also sent by $G$. As messages are conservatively delivered via consensus, the group sequence number of a message is equal to the id of the instance in which it was decided, that is, $m.gs = i \Leftrightarrow \exists I : I.id = i \wedge I.grp = m.src \wedge I.val = m$, where $I.val$ is the value decided by the instance $I$.

There are three possibilities for each message $m$, in the perspective of a process $p$ of $G$:

\begin{itemize}
  \item The message $m$ was originated in $G$, which is the only destination of $m$:
  
  In this case, when $m$ is optimistically delivered by $p$, $p$ checks whether the latest consensus instance $I_{prev}$ in which it participated, or is trying to start, has already been decided -- if not, $p$ enqueues $m$ in a queue $propPending$ as the next message being proposed by it, so other tasks can keep being executed. Then, once $I_{prev}$ has been decided, $p$ checks wheter $m$ -- or a message $m' : m'.ts > m.ts$ -- has already been decided in some previous consensus instance. If such message has indeed been decided previously, $p$ discards $m$; otherwise $p$ proposes $m$ as the next message from $G$. When $m$ is decided, it is not immediately delivered to the application. Instead, it is inserted in a $consPending$ list for later being conservatively delivered, which will happen once every group $G'$ in $sendersTo(G)$ have already sent a message $barrier(G',G) = t$, such that $t > m.ts$. This is done because there could be a message $m'$ yet to come from another group $G'$, such that $m'.ts < m.ts$.

  \item When $m$ is originated in $G$, but it has at least one group other then $G$ as a destination:
  
  In this case, when $now > m.ts + w(p)$, $p$ tries to initiate a consensus instance within $G$ to decide $m$. If $p$ cannot start the proposal now, $m$ is enqueued in $propPending$ for being proposed later. Before proposing $m$, $p$ checks whether $m$ -- or a message $m' : m'.ts > m.ts$ -- has already been decided in some previous consensus instance. If it has, $m$ is discarded; otherwise, $p$ starts a consensus instance proposing $m$. Once any message $m$ is decided, if $G \in m.dst$, $m$ is inserted in the $consPending$ list. Besides, when $m$ is decided, $p$ sends a message $\{m, \text{`cons'}\}$ to every \mbox{$p' : (\exists G' \in m.dst \setminus \{G\}) \wedge (p' \in G')$}. When $\{m, \text{`cons'}\}$ is received by each $p' \in G'$, $p'$ checks whether it has ever inserted $m$ in its own $consPending$ list. If not, $p'$ inserts $m$ into $consPending$ and adjusts $barrier(G, G')$ to $m.ts$. To ensure that, once a message $\{m, \text{`cons'}\}$ is received, every message \mbox{$\{m', \text{`cons'}\}:m'.ts<m.ts$} has already been received from $m.src$, every $\{m, \text{`cons'}\}$ message is sent through a lossless FIFO channel\footnote{An ordinary TCP connection would be enough to provide such FIFO lossless channel.}.
  
  \item When $G$ is one of the destinations of $m$, but $m$ was originated in some other group $G'$:
  
  In this case, when $m$ is optimistically delivered by a process $p$ of group $G$, nothing else is done. The message $m$ is inserted into the \mbox{\textit{consPending}} list of $p$ only when $p$ receives some message $\{m, \text{`cons'}\}$.
\end{itemize}

The messages in the \textit{consPending} list are always sorted in ascending order of their timestamps. When the first message $m$, in the $consPending$ list of a process $p : group(p) = G$, is such that $m.ts < barrier(G, G')$ for all $G' \in sendersTo(G)$, then $m$ is conservatively delivered by $p$ to the application as the next message. We claim that this delivery respects the ``global FIFO total order''\footnote{Proof needed.}.

%The possibility not covered here is when a message is sent from a group to another one which is not its neighbour. However, it is realistic for this context to assume that objects should be close to each other in the virtual environment to interact and that, as such, this kind of interaction is not supported. Nevertheless, one simple way to provide this kind of support could be by each group waiting for the barriers of every other group in the system, instead of only those of its neighbors.

A more formal description of the protocol is given in Algorithm \ref{algorithm:quasi}. We consider that three primitives are given: $getTime()$, which returns the current value of the local wallclock; $Propose(k, val)$, which proposes a value $val$ for the consensus instance of id $k$ within its group; and also $Decide(k, val)$, which is called when the consensus instance of id $k$ finishes. $Decide(k, val)$ is called for all the processes of the group that initiated it, when they learn that the value $val$ has been agreed upon in instance of id $k$. For the sake of simplicity, we assume that, for consensus instances within the same group, the values are decided in the same order of the instances id's\footnote{This can be easily done by delaying the callback of $Decide(k, val)$ while there is some unfinished consensus instance of id $k': k' < k$ from the same group.}.

Moreover, each process $p$ of group $G$ keeps an \textit{optPending} list which contains the messages waiting to be \mbox{\textit{OPT-Delivered}} (optimistically delivered); a \mbox{\textit{propPending}} list containing the messages waiting to be proposed by $p$; and a \mbox{\textit{consPending}} list containing the messages ready to be \textit{CONS-Delivered} (conservatively delivered), but which may be waiting for barriers from the groups in $sendersTo(G)$. Also, each message $m$ has at least four fields: $dst$, which is the set of $m$'s destination groups; $src$, which is the group where $m$ was generated; $ts$, the value of the wallclock of the process when $m$ was generated; and $gs$, which is $m$'s sequence number relatively to its group of origin. Note that, by abuse of notation, we have `process \mbox{$p \in m.dst$'} instead of `process \mbox{$p$ : $\exists$ group $G \in m.dst \wedge p \in G$'}.

Something that must be noticed is that some messages might not have their source groups as a destination. Anyway, each message $m$ of this type still has to be agreed upon in its groups of origin, so that $m.gs$ can be decided, and also for $m$ to be retrievable, even in the presence of failures.% Because of that, to ensure that messages of this kind are also received by $group(p)$'s coordinator $c$, which then initiates the Paxos instance that allows for the conservative delivery of the message in all of its destinations, every message $m$ from $p$ must also be sent to $c$ (l. \ref{algline:sendcoord}). This way, when the clock value at the coordinator $time(c) > w(c) + m.ts$, $c$ will conservatively send $m$ to all the processes in $m.dst$. Finally, to avoid delivering a message $m$ in a group it is not addressed to, before OPT-Delivering it, each process must check whether it is one of $m$'s destinations or not (l. \ref{algline:notinterested}). In l. \ref{algline:sendcoord}, $m$ is sent to the whole group $G$, so that in the case of failure of $G$'s coordinator, it is more likely that the new coordinator will know about the existence of $m$.

\begin{algorithm}
\begin{distribalgo}[1]

\blankline
\INDENT {Initialization}
  \STATE $k \leftarrow 0$, $nextProp \leftarrow 0$, $decided \leftarrow \emptyset$, $propPending \leftarrow \emptyset$, $optPending \leftarrow \emptyset$, $consPending \leftarrow \emptyset$
  \INDENT{\textbf{for all} $G' \in sendersTo(G)$ \textbf{do}}
    \STATE $barrier(G',G) \leftarrow -\infty$ 
  \ENDINDENT
\ENDINDENT 

\blankline
\INDENT{\textit{To send a message $m$} -- \cmsend{m}}
  \STATE $m.ts \leftarrow getTime()$  
  \COMMENT{current wallclock value as the timestamp of $m$}
  \INDENT{\textbf{for all} $p' \in m.dst \cup \{G\}$ \textbf{do}}
    \STATE send($p'$, $m$)    
    \COMMENT{send optimistically $m$ to all involved processes}
  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} receive($m'$)}
  \IF {$m'.ts < getTime() - w(p)$}
    \STATE discard $m'$
    \COMMENT{late commands probably lead to out-of-order delivery}
  \ELSE
    \STATE $optPending \leftarrow optPending$ $\cup$ $\{m'\}$
  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in optPending : getTime() > m.ts + w(p)$ $\wedge\ \nexists m' \in optPending: m'.ts < m.ts$}}
  \STATE $optPending \leftarrow optPending \setminus \{m\}$
  \IF {$G \in m.dst \wedge\ m \neq null$}
    \STATE OPT-Deliver($m$)  
  \ENDIF
  \STATE $propPending \leftarrow propPending \cup \{m\}$
\ENDINDENT

\blankline
\INDENT{\textit{When $(\exists m \in propPending : \nexists m' \in propPending: m'.ts < m.ts) \wedge\ nextProp = k$}}
%  \STATE $propPending \leftarrow propPending \setminus \{m\}$
%  \IF {$m \notin decided \wedge \nexists m' \in decided: m'.ts > m.ts$}
    \STATE $nextProp \leftarrow k + 1$
    \STATE Propose($k$,$m$)
%  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When} Decide($k$,$m$)}
%  \STATE $m.gs \leftarrow k$
  \STATE $decided \leftarrow decided \cup \{m\}$
  \STATE $propPending \leftarrow propPending \setminus (\{m\} \cup \{m' : m'.ts < m.ts \wedge m \neq null\})$ \label{algline:nullstays} 
  \IF {$G \in m.dst \wedge m \neq null$} \label{algline:checkcons}
    \STATE $consPending \leftarrow consPending \cup \{m\}$
  \ENDIF
  \INDENT{\textbf{for all} $p' \in m.dst \setminus \{G\}$ \textbf{do}}
    \STATE send($p'$, $\{m, \text{`cons'}\}$)
    \COMMENT{this message is sent through a FIFO lossless channel}
  \ENDINDENT
  \STATE $nextProp \leftarrow k + 1$
  \STATE $k \leftarrow k + 1$
\ENDINDENT

\blankline
\INDENT{\textit{When} receive($\{m',\{\text{`cons'}\}$)}
  \IF {$m' \notin\ consPending \wedge\ m' \notin\ delivered \wedge\ m' \neq\ null$}
    \STATE $consPending \leftarrow consPending \cup \{m\}$
  \ENDIF
  \STATE $barrier(m'.src,G) \leftarrow max(m'.ts, barrier(m'.src,G))$
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in consPending : \forall G' \in sendersTo(G): m.ts < barrier(G',G)$\\ $\wedge\ \nexists m' \in consPending : m'.ts < m.ts$ }}
  \STATE $consPending \leftarrow consPending \setminus \{m\}$
  \STATE CONS-Deliver($m$)
  \STATE $delivered \leftarrow delivered \cup \{m\}$

\blankline
\ENDINDENT

\caption{\cmsend{m} -- executed by every process $p$ from group $G$}
\label{algorithm:quasi}
\end{distribalgo}
\end{algorithm}

\subsubsection{Addressing liveness}

The problem with Algorithm \ref{algorithm:quasi} is that it does not guarantee liveness when a group has no message to receive from some other group and then keeps waiting for a new message to increase the barrier value and proceed with the conservative delivery. However, it does not disrupt the optimistic delivery. Also, liveness for the conservative delivery can be easily provided by sending periodic empty messages from $G$ to each $G' \in receiversFrom(G)$ to which no message has been sent for a specified time threshold \textit{barrierThreshold}. Algorithm \ref{algorithm:liveness} describes this. When a message from group $G$ to some other group $G'$ has been agreed upon and a $\{m, \text{`cons'}\}$ message is sent by each process $p$ of $G$ to every process $p'$ of $G'$, $p$ knows that $p'$ increased its barrier value for messages from $G$. However, when there is a long period after the last time such kind of message has been sent, $p$ decides to send some empty message to the processes of $G'$ with the sole purpose of increasing their barrier values and allow for the delivery of possibly blocked messages in $G'$.

\begin{algorithm}
\begin{distribalgo}[1]
\blankline
\INDENT {Initialization}
  \INDENT{\textbf{for all} $G' \in receiversFrom(G)$ \textbf{do}}
    \STATE $lastBarrierSent(G') = -\infty$
  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} send($p'$,$\{m, \text{`cons'}\}$) for every $p' \in G'$}
  \STATE $lastBarrierSent(G') \leftarrow m.ts$
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists G' \in receiversFrom(G):$\\$getTime() - lastBarrierSent(G') > barrierThreshold$}}
  \STATE $null \leftarrow$ empty message
  \STATE $null.ts \leftarrow lastBarrierSent(G') + barrierThreshold$ \label{algline:samenullid}
  \STATE $null.src \leftarrow G$
  \STATE $null.dst \leftarrow G'$
  \STATE \cmsend{$null$}
  \COMMENT{saving that nothing was sent until null.ts}
\ENDINDENT 

\blankline
\caption{Achieving liveness; executed by every process $p$ of group $G$}
\label{algorithm:liveness}
\end{distribalgo}
\end{algorithm}

The problem with addressing liveness this way is that, in the worst case, $G'$ has decided a message $m$ and has just received the last barrier $b$ from $G$, such that $b$ is less than $m.ts$ by an infinitesimal difference. This would mean that, if $G$ has no messages to send to $G'$, $G'$ will have to wait at least for $barrierThreshold + \tconsm$, where \tcons\ is the time needed for a value to be decided after it has been proposed in some consensus instance -- assuming that such value is going to be decided at all in that instance. Because of that, the conservative delivery of $m$ will be delayed by $w(p) + barrierThreshold + 2\tconsm$, where $w(p)$ is the wait window of the process $p \in G$, and \tcons\ is counted twice because there are two consensus proposals -- first in $G'$ regarding $m$, and then in $G$ regarding the empty message.

Another problem with this approach is that it is necessary to guarantee that a $null$ message will eventually be proposed, decided, and a barrier will be sent to some group which might be needing it, so that progression is guaranteed. Therefore, this kind of messages are excluded from the $propPending$ list only when decided in the group (l. \ref{algline:nullstays} of Algorithm \ref{algorithm:quasi}), since if at least one of the processes does not have it, it could never be decided. On the other hand, this brings another problem: if a $null$ message in the $propPending$ list of some process is never removed unless decided, its presence could prevent other messages of being proposed. To circumvent this, we assume that no two different messages have the same timestamp, that is, $m \neq m' \Leftrightarrow m.ts \neq m'.ts$. Now, we only have to ensure that

There is a way to provide liveness withouth such a possibly big delay, although creating more messages and making deeper changes in the delivery algorithm. Let $blockers(m)$ be defined as the set of groups whose barrier is needed in order to some group to deliver $m$. More formally $blockers(m) =$ \mbox{$\{G_B : \exists G_{dst} \in m.dst \wedge G_B \in sendersTo(G_{dst})\}$}. The idea is that, once each group $G_B$ in $blockers(m)$ have sent a barrier $b > m.ts$ to all the groups belonging to \mbox{$m.dst \cap receiversFrom(G_B)$}, all possible destinations of $m$ can deliver it. This way, instead of relying on periodic messages, whenever a process $p$ in a group $G$ has a message $m$ to send, $p$ sends $m$ to every $p' \in m.dst \cup G \cup blockers(m)$. It is sent to the groups in $blockers(m)$, so that they know that there is a message which will be blocked until they send a proper barrier to unblock it.

When the process $p'$ of some group $G'$ receives $m$, such that $m.src \neq G'$, $p'$ knows that there might be other groups depeding on the barrier of $G'$ to deliver $m$. For that reason, it will immediately create a $null$ message with a timestamp equals to $m.ts$ and with $null.dst = m.dst$ $\cap$ $receiversFrom(G')$. Then, $p$ will execute \mbox{\cmsend{$null$}}. As soon as $null.ts > now - w(p')$, the $null$ message will be proposed in $G'$ and, once $null$ is decided, each process $p'$ of $G'$ will send a $\{null, \text{`cons'}\}$ message to every process in every group \mbox{$G \in m.dst \cap receiversFrom(G')$}. This way, any group which was waiting for a barrier from $G'$ to deliver $m$ will be able to do so as soon as it receives such $null$ message. The new delivery algorithm would be as described in \mbox{Algorithm {\ref{algorithm:ondemand}}}.

\begin{algorithm}
\begin{distribalgo}[1]

\blankline
\INDENT {Initialization}
  \STATE $k \leftarrow 0$, $nextProp \leftarrow 0$, $decided \leftarrow \emptyset$, $propPending \leftarrow \emptyset$, $optPending \leftarrow \emptyset$, $consPending \leftarrow \emptyset$
  \INDENT{\textbf{for all} $G' \in sendersTo(G)$ \textbf{do}}
    \STATE $barrier(G',G) \leftarrow -\infty$ 
  \ENDINDENT
\ENDINDENT 

\blankline
\INDENT{\textit{To send a message $m$} -- \cmsend{m}}
  \STATE $m.ts \leftarrow getTime()$  
  \COMMENT{current wallclock value as the timestamp of $m$}
  \INDENT{\textbf{for all} $p' \in m.dst \cup \{G\} \cup blockers(m)$ \textbf{do}}
    \STATE send($p'$, $m$)    
    \COMMENT{send optimistically $m$ to all involved processes}
  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} receive($m'$)}
  \IF {$G \neq m'.src$}
    \STATE $null \leftarrow$ empty message
    \STATE $null.src \leftarrow G$
    \STATE $null.ts \leftarrow m'.ts$
    \STATE $null.dst \leftarrow m'.dst \cap receiversFrom(G)$
    \STATE $optPending \leftarrow optPending \cup \{null\}$
  \ENDIF
  \IF {$m'.ts < getTime() - w(p) \vee G \notin m'.dst$}
    \STATE discard $m'$
    \COMMENT{late commands probably lead to out-of-order delivery}
  \ELSE
    \STATE $optPending \leftarrow optPending$ $\cup$ $\{m'\}$
  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in optPending : getTime() > m.ts + w(p)$ $\wedge\ \nexists m' \in optPending: m'.ts < m.ts$}}
  \STATE $optPending \leftarrow optPending \setminus \{m\}$
  \IF {$G \in m.dst \wedge\ m \neq null$}
    \STATE OPT-Deliver($m$)  
  \ENDIF
  \STATE $propPending \leftarrow propPending \cup \{m\}$
\ENDINDENT

\blankline
\INDENT{\textit{When $(\exists m \in propPending : \nexists m' \in propPending: m'.ts < m.ts) \wedge\ nextProp = k$}}
%  \STATE $propPending \leftarrow propPending \setminus \{m\}$
%  \IF {$m \notin decided \wedge \nexists m' \in decided: m'.ts > m.ts$}
    \STATE $nextProp \leftarrow k + 1$
    \STATE Propose($k$,$m$)
%  \ENDIF
\ENDINDENT

\blankline
\INDENT{\textit{When} Decide($k$,$m$)}
%  \STATE $m.gs \leftarrow k$
  \STATE $decided \leftarrow decided \cup \{m\}$
  \STATE $propPending \leftarrow propPending \setminus (\{m\} \cup \{m' : m'.ts < m.ts \wedge m' \neq null\})$ \label{algline:keepnull}
  \STATE $nextProp \leftarrow k + 1$
  \STATE $k \leftarrow k + 1$  
  \IF {$G \in m.dst$} \label{algline:checkcons}
    \STATE $consPending \leftarrow consPending \cup \{m\}$
  \ENDIF
  \INDENT{\textbf{for all} $p' \in m.dst \setminus \{G\}$ \textbf{do}}
    \STATE send($p'$, $\{m, \text{`cons'}\}$)    
  \ENDINDENT
\ENDINDENT

\blankline
\INDENT{\textit{When} receive($\{m',\{\text{`cons'}\}$)}  
  \IF {$m' \notin\ consPending \wedge\ m' \notin\ delivered \wedge\ m' \neq\ null$}
    \STATE $consPending \leftarrow consPending \cup \{m\}$
  \ENDIF
  \STATE $barrier(m'.src,G) \leftarrow max(m'.ts, barrier(m'.src,G))$
\ENDINDENT

\blankline
\INDENT{\textit{When $\exists m \in consPending : \forall G' \in sendersTo(G): m.ts < barrier(G',G)$\\ $\wedge\ \nexists m' \in consPending : m'.ts < m.ts$ }}
  \STATE $consPending \leftarrow consPending \setminus \{m\}$
  \STATE CONS-Deliver($m$)
  \STATE $delivered \leftarrow delivered \cup \{m\}$

\blankline
\ENDINDENT

\caption{\cmsend{m} -- executed by every process $p$ from group $G$}
\label{algorithm:ondemand}
\end{distribalgo}
\end{algorithm}

Now, assuming that $w(p)$ is the same for every process $p$, we have a delivery latency for $m$ of $w(p) + \tconsm$, as the $null$ message is created and proposed in parallel with $m$. However, each sender process is sending $m$ not only to every destination process, but also to every process in its group and to every process belonging to some group of $blockers(m)$. This can create a fairly high amount of messages. Unfortunately, this has to be done to guarantee that such $null$ message will eventually be proposed, decided, and a barrier will be sent to some group which might be needing it. Although this kind of messages are excluded from the $propPending$ list only when decided in the group (l. \ref{algline:keepnull} of Algorithm \ref{algorithm:ondemand}), if at least one of the processes does not have it, it could never be decided.

% Nevertheless, one possible way to implement the delivery for a command $C$ of this type would be to initiate a Paxos consensus instance in each group addressed by it, proposing $C$ and having as learns all servers in all involved groups. 

%In brief, our strategy is to combine state-machine replication (implemented with Paxos), used to handle object replicas, with Skeen's algorithm, used to select timestamps for commands that affect multiple objects.

\subsection{Recovering from mistakes}

Unfortunately, even with a very good delay estimation (e.g. on an environment with a low jitter), there is absolutely no guarantee that the multicast protocol described  in section \ref{sec:quasi} will deliver the game command messages optimistically and conservatively in the same order. When it doesn't, it is considered a \emph{mistake}. Every mistake of the optimistic delivery -- either a lost command message, or an out-of-order delivery -- will cause a rollback of the optimistic state of the objects and re-execution of some of the optimistically delivered commands.

To perform that, we consider that each object has an optimistic delivery queue, $Q_{opt}$. Whenever a command is optimistically delivered, the optimistic state is updated and the command is pushed in the back of $Q_{opt}$. Whenever a command $C_c$ is conservatively delivered, it updates the conservative state of each object in $obj(C_c)$ and, for each one of them, the algorithm checks whether it is the first command in $Q_{opt}$. If it is, $C_c$ is simply removed from $Q_{opt}$ and the execution continues. If it isn't, it means that $C_c$ was either optimistically delivered out of order, or it was simply never optmistically delivered. It then checks whether $Q_{opt}$ contains $C_c$. If it does, it means the command was optimistically deliverd out of order, and it is removed from the list -- if $Q_{opt}$ doesn't contain $C_c$, it was probably lost\footnote{Also, when $C_c$ is delivered, but it is not in $Q_{opt}$, the remaining possibility is the very unlikely case where the conservative delivery happened before the optimistic one. To handle this case, $C_c$ is stored in a list of possibly delayed optimistic delivery and, if it is ever optimistically delivered, the algorithm will know that it should only discard that command, instead of updating the optimistic state.}. Then, the optimistic state is overwritten with the conservative one and, from that state, all the remaining comands in $Q_{opt}$ are re-executed, leading to a new optimistic state for that object.


%\bibliographystyle{latex8}
%\bibliography{main}
\bibliographystyle{ieeetr}
\bibliography{gftommog}
\end{document}

